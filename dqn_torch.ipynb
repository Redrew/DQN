{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn-torch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "43hhU862TGxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torchvision\n",
        "import torchvision.transforms as tfms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsGFqvqKcAtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Feedback = namedtuple('feedback', ['state', 'reward', 'done', 'info'])\n",
        "Record = namedtuple('record', ['action', 'state', 'reward', 'done'])\n",
        "Transition = namedtuple('transition', ['state', 'action', 'next_state', 'reward', 'done'])\n",
        "\n",
        "class Transform:\n",
        "  def __init__(self):\n",
        "    self.img_transform = tfms.Compose([\n",
        "      tfms.ToPILImage(),\n",
        "      tfms.Grayscale(),\n",
        "      tfms.Resize((42, 55)),\n",
        "      tfms.CenterCrop(42),\n",
        "      tfms.ToTensor()\n",
        "    ])\n",
        "  \n",
        "  def __call__(self, framestack):\n",
        "    imgs = []\n",
        "    for stack in framestack:\n",
        "      img = self.img_transform(stack)[0]\n",
        "      imgs.append(img)\n",
        "    return torch.stack(imgs)\n",
        "\n",
        "class ExpBuffer:\n",
        "  def __init__(self, max_size = 100000):\n",
        "    self.max_size = 100000\n",
        "    self.records = []\n",
        "    self.state_shape = None\n",
        "  \n",
        "  def add_record(self, action, state, reward, done):\n",
        "    if len(self.records) == self.max_size: self.records.pop(0)\n",
        "    self.state_shape = state.shape\n",
        "    self.records.append(Record(action, state, reward, done))\n",
        "  \n",
        "  def add_state(self, state):\n",
        "    self.add_record(None, state, None, False)\n",
        "\n",
        "  def sample(self, batch_size, device='cpu'):\n",
        "    if len(self.records) <= 1: raise Error('Sampling before buffer is filled') \n",
        "    states = torch.zeros(batch_size, *buffer.state_shape)\n",
        "    actions = torch.zeros(batch_size, dtype=torch.long)\n",
        "    next_states = torch.zeros(batch_size, *buffer.state_shape)\n",
        "    rewards = torch.zeros(batch_size)\n",
        "    done = torch.zeros(batch_size, dtype=torch.bool)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      while True:\n",
        "        idx = np.random.randint(0, len(self.records))\n",
        "        if idx != 0 and self.records[idx].action is not None: break\n",
        "      record = self.records[idx]\n",
        "      states[i] = self.records[idx-1].state\n",
        "      actions[i] = record.action\n",
        "      rewards[i] = record.reward\n",
        "      if record.done: done[i] = True\n",
        "      else: next_states[i] = record.state\n",
        "\n",
        "    return Transition(states.to(device), actions.to(device), \n",
        "                      next_states.to(device), rewards.to(device), \n",
        "                      done.to(device))\n",
        "    \n",
        "    \n",
        "    return Transition(self.records[idx-1].state, *self.records[idx])\n",
        "    \n",
        "  \n",
        "  def __len__(self): return len(self.records)\n",
        "\n",
        "def get_DQN(nb_actions): \n",
        "  return nn.Sequential(\n",
        "    # 42 * 42\n",
        "    nn.Conv2d(4, 16, 7, 2, 3), # 21 * 21\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2), # 10 * 10\n",
        "    nn.Conv2d(16, 64, 3, 2, 1), # 5 * 5\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2), # 2 * 2\n",
        "    nn.Flatten(), # 2 * 2 * 32\n",
        "    nn.Linear(2 * 2 * 64, 256, True),\n",
        "    nn.BatchNorm1d(256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, nb_actions)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbzQ5pZ4971z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action():\n",
        "  if np.random.rand() < args.epsilon:\n",
        "    action = np.random.randint(nb_actions)\n",
        "  else:\n",
        "    model.eval()\n",
        "    Q = model(state.unsqueeze(0).to(device))\n",
        "    action = torch.max(Q, 1)[1].item()\n",
        "\n",
        "def optimize():\n",
        "  # train using past transitions\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  # randomly sample a batch of transitions from buffer\n",
        "  batch = buffer.sample(args.batch_size, device)\n",
        "\n",
        "  # predicted value of next state\n",
        "  Qs = model(batch.state)\n",
        "  outputs = Qs[np.arange(args.batch_size), batch.action]\n",
        "  # estimated value of next state\n",
        "  next_Qs = torch.max(model(batch.next_state), 1)[0]\n",
        "  targets = args.gamma * batch.reward + next_Qs * ~batch.done\n",
        "\n",
        "  # calculate loss\n",
        "  loss = loss_func(outputs, targets)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCmLkaucXOzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args: pass\n",
        "args = Args()\n",
        "args.lr = 0.003\n",
        "args.l2reg = 0.0003\n",
        "args.episodes = 10\n",
        "args.max_steps = 150\n",
        "args.epsilon = 0.1\n",
        "args.batch_size = 32\n",
        "args.gamma = 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIeydMK4-4dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = Transform()\n",
        "buffer = ExpBuffer()\n",
        "\n",
        "env = gym.make('Pong-v0')\n",
        "env = gym.wrappers.FrameStack(env, 4)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "model = get_DQN(nb_actions)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), args.lr)\n",
        "# Huber Loss: equivalent to MSE when difference is small, but less punishing\n",
        "# when the difference is large, resilient to outliers\n",
        "loss_func = F.smooth_l1_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGFNDh5pYI84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a819c36f-0489-45eb-8754-7b83f9dc5120"
      },
      "source": [
        "%%prun\n",
        "for episode_idx in range (args.episodes):\n",
        "  state = env.reset()\n",
        "  state = transform(state)\n",
        "  buffer.add_state(state)\n",
        "  score = 0\n",
        "\n",
        "  for t in range(args.max_steps):\n",
        "    # calculate next action using epsilon-greedy\n",
        "    get_action()\n",
        "\n",
        "    # step\n",
        "    state, reward, done, info = env.step(action)\n",
        "    state = transform(state)\n",
        "    buffer.add_record(action, state, reward, done)\n",
        "    score += reward\n",
        "    \n",
        "    optimize()\n",
        "  \n",
        "  print('episode %d, score %f' % (episode_idx, score))\n",
        "    "
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0, score -2.000000\n",
            "episode 1, score -2.000000\n",
            "episode 2, score -2.000000\n",
            "episode 3, score -2.000000\n",
            "episode 4, score -2.000000\n",
            "episode 5, score -2.000000\n",
            "episode 6, score -2.000000\n",
            "episode 7, score -2.000000\n",
            "episode 8, score -2.000000\n",
            "episode 9, score -2.000000\n",
            " "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}